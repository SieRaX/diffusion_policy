{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a squence estimator that gets observation and time as input, and outputs predicted attention for time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Make dataset\n",
    "# 2. Make model\n",
    "# 3. Train model\n",
    "# 4. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusion_policy.dataset.robomimic_replay_image_dataset import RobomimicReplayImageDataset\n",
    "from diffusion_policy.dataset.robomimic_replay_lowdim_dataset import RobomimicReplayLowdimDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading hdf5 to ReplayBuffer: 100%|██████████| 200/200 [00:00<00:00, 1291.37it/s]\n",
      "Loading hdf5 to ReplayBuffer: 100%|██████████| 200/200 [00:00<00:00, 1295.63it/s]\n"
     ]
    }
   ],
   "source": [
    "lowdim_dataset_path = \"../data/robomimic/datasets/lift/ph/low_dim_abs_with_sine_wave.hdf5\"\n",
    "lowdim_dataset = RobomimicReplayLowdimDataset(lowdim_dataset_path, horizon=16, pad_before=1, pad_after=7, obs_keys=['object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos', 'spatial_attention'], abs_action=True, rotation_rep='rotation_6d', use_legacy_normalizer=False, seed=42, val_ratio=0.02, max_train_episodes=None)\n",
    "val_dataset = lowdim_dataset.get_validation_dataset()\n",
    "\n",
    "train_dataloader = DataLoader(lowdim_dataset, batch_size=256, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "display_dataloader = DataLoader(\n",
    "    RobomimicReplayLowdimDataset(lowdim_dataset_path, horizon=16, pad_before=1, pad_after=7, obs_keys=['object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos', 'spatial_attention'], abs_action=True, rotation_rep='rotation_6d', use_legacy_normalizer=False, seed=42, val_ratio=0.0, max_train_episodes=None),\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "normalizer = lowdim_dataset.get_normalizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AttentionEstimator(nn.Module):\n",
    "#     def __init__(self, input_dim=19, output_dim=1, hidden_dim=256):\n",
    "#         super().__init__()\n",
    "#         t_embedding_dim = 8\n",
    "#         self.input_dim = input_dim + t_embedding_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         # Define the network architecture\n",
    "#         self.fc1 = nn.Linear(input_dim+t_embedding_dim, hidden_dim)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.fc5 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "#         # Initialize weights\n",
    "#         self._init_weights()\n",
    "    \n",
    "#     def _init_weights(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "#                 nn.init.zeros_(m.bias)\n",
    "    \n",
    "#     def forward(self, x, t):\n",
    "#         # Input shape: (batch_size, input_dim)\n",
    "#         # Time embedding\n",
    "#         t = t.unsqueeze(-1)\n",
    "#         t_embedding = torch.cat([\n",
    "#             torch.sin(t * 0.1),\n",
    "#             torch.cos(t * 0.1),\n",
    "#             torch.sin(t * 1.0), \n",
    "#             torch.cos(t * 1.0),\n",
    "#             torch.sin(t * 10.0),\n",
    "#             torch.cos(t * 10.0),\n",
    "#             torch.sin(t * 100.0),\n",
    "#             torch.cos(t * 100.0)\n",
    "#         ], dim=-1).squeeze()\n",
    "        \n",
    "#         # Combine input with time embedding\n",
    "#         x = torch.cat([x, t_embedding], dim=-1)\n",
    "        \n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         x = F.relu(self.fc4(x))\n",
    "#         x = self.fc5(x)\n",
    "#         return x\n",
    "    \n",
    "class AttentionEstimator(nn.Module):\n",
    "    def __init__(self, input_dim=19, output_dim=16, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define the network architecture\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, input_dim)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample = next(iter(train_dataloader))\n",
    "\n",
    "Doutput = train_data_sample['obs'][0, :, -1].shape[0]\n",
    "Dinput = train_data_sample['obs'][0, :, :-1].shape[-1] * 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1567730903625488\n",
      "0.36465224623680115\n",
      "0.14447760581970215\n",
      "0.17161902785301208\n",
      "0.18809431791305542\n",
      "0.15808561444282532\n",
      "0.10799244046211243\n",
      "0.07327806949615479\n",
      "0.0611211322247982\n",
      "0.06335584074258804\n",
      "0.07157712429761887\n",
      "0.07539542019367218\n",
      "0.0678180381655693\n",
      "0.060634300112724304\n",
      "0.045620713382959366\n",
      "0.041638851165771484\n",
      "0.04354478418827057\n",
      "0.035821110010147095\n",
      "0.04296157509088516\n",
      "0.041265878826379776\n",
      "0.037809934467077255\n",
      "0.03724417835474014\n",
      "0.030226420611143112\n",
      "0.02517133578658104\n",
      "0.025644008070230484\n",
      "0.023493144661188126\n",
      "0.02404269017279148\n",
      "0.034703828394412994\n",
      "0.026989899575710297\n",
      "0.024028806015849113\n",
      "0.01716865971684456\n",
      "0.019497008994221687\n",
      "Epoch 0 train loss: 0.019497008994221687, val loss: 0.017394643276929855\n",
      "0.026406198740005493\n",
      "0.02396572381258011\n",
      "0.01519162766635418\n",
      "0.015011955052614212\n",
      "0.013474885374307632\n",
      "0.019184483215212822\n",
      "0.017397677525877953\n",
      "0.013986242935061455\n",
      "0.013981920666992664\n",
      "0.01363581232726574\n",
      "0.012863699346780777\n",
      "0.013922341167926788\n",
      "0.013352837413549423\n",
      "0.017269058153033257\n",
      "0.018985334783792496\n",
      "0.020153993740677834\n",
      "0.03017386421561241\n",
      "0.01471912581473589\n",
      "0.015093551948666573\n",
      "0.017216430976986885\n",
      "0.01740204729139805\n",
      "0.014596971683204174\n",
      "0.013623103499412537\n",
      "0.01049298420548439\n",
      "0.013147289864718914\n",
      "0.013539547100663185\n",
      "0.015600931830704212\n",
      "0.019221005961298943\n",
      "0.011857817880809307\n",
      "0.01181408017873764\n",
      "0.011089762672781944\n",
      "0.012210898101329803\n",
      "Epoch 1 train loss: 0.012210898101329803, val loss: 0.008977665565907955\n",
      "0.01677638106048107\n",
      "0.01958347111940384\n",
      "0.007774720899760723\n",
      "0.009719235822558403\n",
      "0.0076919663697481155\n",
      "0.011983038857579231\n",
      "0.013140710070729256\n",
      "0.01047303806990385\n",
      "0.009699777700006962\n",
      "0.010514261201024055\n",
      "0.009337341412901878\n",
      "0.009442873299121857\n",
      "0.009567348286509514\n",
      "0.012748494744300842\n",
      "0.015233535319566727\n",
      "0.015245024114847183\n",
      "0.023005899041891098\n",
      "0.010911456309258938\n",
      "0.01136801578104496\n",
      "0.01310482807457447\n",
      "0.01252212468534708\n",
      "0.009186365641653538\n",
      "0.009595319628715515\n",
      "0.008020306937396526\n",
      "0.011410227976739407\n",
      "0.010991240851581097\n",
      "0.011587429791688919\n",
      "0.01568092778325081\n",
      "0.008705994114279747\n",
      "0.008199058473110199\n",
      "0.009200421161949635\n",
      "0.009902628138661385\n",
      "Epoch 2 train loss: 0.009902628138661385, val loss: 0.0076359896920621395\n",
      "0.015055171214044094\n",
      "0.015619559213519096\n",
      "0.007060792297124863\n",
      "0.007029169704765081\n",
      "0.0058549498207867146\n",
      "0.009792419150471687\n",
      "0.011961746960878372\n",
      "0.008811834268271923\n",
      "0.008685028180480003\n",
      "0.009453019127249718\n",
      "0.008521169424057007\n",
      "0.008088254369795322\n",
      "0.007947521284222603\n",
      "0.010292131453752518\n",
      "0.013887858018279076\n",
      "0.012628157623112202\n",
      "0.019090548157691956\n",
      "0.010037554427981377\n",
      "0.009287106804549694\n",
      "0.011122321709990501\n",
      "0.00899461004883051\n",
      "0.007506885100156069\n",
      "0.008336141705513\n",
      "0.006216882728040218\n",
      "0.0101191196590662\n",
      "0.009370120242238045\n",
      "0.010448416694998741\n",
      "0.0135327884927392\n",
      "0.006878536194562912\n",
      "0.006690719164907932\n",
      "0.009326535277068615\n",
      "0.008358804509043694\n",
      "Epoch 3 train loss: 0.008358804509043694, val loss: 0.007261894177645445\n",
      "0.013287849724292755\n",
      "0.013248519971966743\n",
      "0.006360939703881741\n",
      "0.0053620971739292145\n",
      "0.0053503140807151794\n",
      "0.008796059526503086\n",
      "0.011374074034392834\n",
      "0.007968972437083721\n",
      "0.008244347758591175\n",
      "0.008609417825937271\n",
      "0.00854373537003994\n",
      "0.007370826322585344\n",
      "0.007310490123927593\n",
      "0.008442813530564308\n",
      "0.013428069651126862\n",
      "0.011020145379006863\n",
      "0.016615793108940125\n",
      "0.00943708699196577\n",
      "0.008021434769034386\n",
      "0.009519838728010654\n",
      "0.007569830399006605\n",
      "0.00669087516143918\n",
      "0.0075837308540940285\n",
      "0.00563014205545187\n",
      "0.010230440646409988\n",
      "0.008587111718952656\n",
      "0.009856424294412136\n",
      "0.011941193602979183\n",
      "0.005799907259643078\n",
      "0.005630095023661852\n",
      "0.00955005269497633\n",
      "0.00730128213763237\n",
      "Epoch 4 train loss: 0.00730128213763237, val loss: 0.00672559579834342\n",
      "0.012285205535590649\n",
      "0.011646445840597153\n",
      "0.006324725225567818\n",
      "0.004993231035768986\n",
      "0.005079865455627441\n",
      "0.008007146418094635\n",
      "0.011224615387618542\n",
      "0.007829143665730953\n",
      "0.008099897764623165\n",
      "0.00794913712888956\n",
      "0.008164201863110065\n",
      "0.007270206697285175\n",
      "0.007170343305915594\n",
      "0.007110961247235537\n",
      "0.01271310169249773\n",
      "0.010564077645540237\n",
      "0.01489238627254963\n",
      "0.00871410220861435\n",
      "0.007497559767216444\n",
      "0.00886978767812252\n",
      "0.006462275981903076\n",
      "0.0059026191011071205\n",
      "0.006955188233405352\n",
      "0.005036209709942341\n",
      "0.009239628911018372\n",
      "0.007774807512760162\n",
      "0.009645872749388218\n",
      "0.010525057092308998\n",
      "0.004984310362488031\n",
      "0.004977599252015352\n",
      "0.009529897943139076\n",
      "0.006640964653342962\n",
      "Epoch 5 train loss: 0.006640964653342962, val loss: 0.0064618573524057865\n",
      "0.011375678703188896\n",
      "0.010606260970234871\n",
      "0.006454531569033861\n",
      "0.004493699409067631\n",
      "0.004883709829300642\n",
      "0.007393217645585537\n",
      "0.010770546272397041\n",
      "0.0074465894140303135\n",
      "0.007663574535399675\n",
      "0.007370682433247566\n",
      "0.007889891974627972\n",
      "0.006931433454155922\n",
      "0.0068885572254657745\n",
      "0.00655609555542469\n",
      "0.011857422068715096\n",
      "0.009433452039957047\n",
      "0.013791331090033054\n",
      "0.008159221149981022\n",
      "0.006796585395932198\n",
      "0.007995226420462132\n",
      "0.005814232397824526\n",
      "0.005709751509130001\n",
      "0.0068290140479803085\n",
      "0.004689151421189308\n",
      "0.008808786049485207\n",
      "0.007373340893536806\n",
      "0.00919284950941801\n",
      "0.009529806673526764\n",
      "0.004525850992649794\n",
      "0.00474280770868063\n",
      "0.009029577486217022\n",
      "0.006180856842547655\n",
      "Epoch 6 train loss: 0.006180856842547655, val loss: 0.006874441169202328\n",
      "0.011234747245907784\n",
      "0.00960494950413704\n",
      "0.006467507220804691\n",
      "0.004186314530670643\n",
      "0.004911218769848347\n",
      "0.006858528591692448\n",
      "0.010590012185275555\n",
      "0.007190001662820578\n",
      "0.007179634179919958\n",
      "0.0068318541161715984\n",
      "0.007783928886055946\n",
      "0.006429963745176792\n",
      "0.006601500790566206\n",
      "0.005979499313980341\n",
      "0.01127494964748621\n",
      "0.008839835412800312\n",
      "0.013247988186776638\n",
      "0.007842374965548515\n",
      "0.0062939622439444065\n",
      "0.007272673305124044\n",
      "0.00545188132673502\n",
      "0.005508528556674719\n",
      "0.006693671457469463\n",
      "0.004451640881597996\n",
      "0.008486594073474407\n",
      "0.007162893656641245\n",
      "0.008963555097579956\n",
      "0.008682884275913239\n",
      "0.004204501863569021\n",
      "0.004497601184993982\n",
      "0.008959089405834675\n",
      "0.005686004646122456\n",
      "Epoch 7 train loss: 0.005686004646122456, val loss: 0.006418558768928051\n",
      "0.010673679411411285\n",
      "0.008811373263597488\n",
      "0.006420471239835024\n",
      "0.0038547811564058065\n",
      "0.004641267471015453\n",
      "0.0066719381138682365\n",
      "0.010188077576458454\n",
      "0.0070752790197730064\n",
      "0.006843181326985359\n",
      "0.006654903292655945\n",
      "0.007746157236397266\n",
      "0.005927125923335552\n",
      "0.006516703404486179\n",
      "0.005866352468729019\n",
      "0.01071606669574976\n",
      "0.00838003121316433\n",
      "0.01291551161557436\n",
      "0.00775666069239378\n",
      "0.006025600712746382\n",
      "0.006878364831209183\n",
      "0.00529533950611949\n",
      "0.005134412087500095\n",
      "0.0065306113101542\n",
      "0.004171526525169611\n",
      "0.00792242493480444\n",
      "0.006721857935190201\n",
      "0.008809538558125496\n",
      "0.008007204160094261\n",
      "0.003976227715611458\n",
      "0.004294727463275194\n",
      "0.008361509069800377\n",
      "0.005388947203755379\n",
      "Epoch 8 train loss: 0.005388947203755379, val loss: 0.006546369753777981\n",
      "0.01033483725041151\n",
      "0.008160912431776524\n",
      "0.006434279959648848\n",
      "0.0037537591997534037\n",
      "0.004560457542538643\n",
      "0.006602682638913393\n",
      "0.010044732131063938\n",
      "0.006933640222996473\n",
      "0.006573420949280262\n",
      "0.006468837149441242\n",
      "0.007703117560595274\n",
      "0.005624628625810146\n",
      "0.0065843770280480385\n",
      "0.0057507287710905075\n",
      "0.010256271809339523\n",
      "0.00808855053037405\n",
      "0.012583330273628235\n",
      "0.00766635499894619\n",
      "0.005778319668024778\n",
      "0.0064192055724561214\n",
      "0.005124004557728767\n",
      "0.005160365253686905\n",
      "0.006338491104543209\n",
      "0.003952403087168932\n",
      "0.007796675432473421\n",
      "0.006524562370032072\n",
      "0.00861315242946148\n",
      "0.007477710954844952\n",
      "0.003890848718583584\n",
      "0.004344483371824026\n",
      "0.008195634931325912\n",
      "0.005091962404549122\n",
      "Epoch 9 train loss: 0.005091962404549122, val loss: 0.006400845944881439\n",
      "0.009802529588341713\n",
      "0.00770413875579834\n",
      "0.006276187486946583\n",
      "0.003642578376457095\n",
      "0.004533267579972744\n",
      "0.006294051185250282\n",
      "0.00988115556538105\n",
      "0.0069149211049079895\n",
      "0.006320955231785774\n",
      "0.006309410557150841\n",
      "0.00757282180711627\n",
      "0.005175914149731398\n",
      "0.006521850824356079\n",
      "0.005803569685667753\n",
      "0.010030202567577362\n",
      "0.007865985855460167\n",
      "0.012216477654874325\n",
      "0.0075577921234071255\n",
      "0.0054550496861338615\n",
      "0.006193438079208136\n",
      "0.005006901919841766\n",
      "0.004992500878870487\n",
      "0.006373188458383083\n",
      "0.003842859296128154\n",
      "0.007416622247546911\n",
      "0.006176583468914032\n",
      "0.008319017477333546\n",
      "0.006836100481450558\n",
      "0.0037285552825778723\n",
      "0.004274529404938221\n",
      "0.008066043257713318\n",
      "0.004868697840720415\n",
      "Epoch 10 train loss: 0.004868697840720415, val loss: 0.00642557954415679\n",
      "0.009341071359813213\n",
      "0.007112570572644472\n",
      "0.006132917013019323\n",
      "0.0034470288082957268\n",
      "0.004479493945837021\n",
      "0.0062480587512254715\n",
      "0.009658467024564743\n",
      "0.006895547732710838\n",
      "0.006135367322713137\n",
      "0.00619432982057333\n",
      "0.007473847828805447\n",
      "0.00484527088701725\n",
      "0.006481121759861708\n",
      "0.005997851025313139\n",
      "0.009667130187153816\n",
      "0.007582449354231358\n",
      "0.012044943869113922\n",
      "0.007554945535957813\n",
      "0.0053878528997302055\n",
      "0.006016138009727001\n",
      "0.004953258205205202\n",
      "0.0049071162939071655\n",
      "0.006548892706632614\n",
      "0.0038152430206537247\n",
      "0.007509805262088776\n",
      "0.006278508342802525\n",
      "0.008057364262640476\n",
      "0.006394586525857449\n",
      "0.00386459706351161\n",
      "0.004324742592871189\n",
      "0.00821162573993206\n",
      "0.0048460871912539005\n",
      "Epoch 11 train loss: 0.0048460871912539005, val loss: 0.00665505975484848\n",
      "0.009247428737580776\n",
      "0.006780650932341814\n",
      "0.0061883036978542805\n",
      "0.0034749398473650217\n",
      "0.004515704698860645\n",
      "0.006373753305524588\n",
      "0.00946106854826212\n",
      "0.007023606449365616\n",
      "0.00613885885104537\n",
      "0.006041001528501511\n",
      "0.007273642811924219\n",
      "0.00470913527533412\n",
      "0.006412496790289879\n",
      "0.0060453349724411964\n",
      "0.009571813978254795\n",
      "0.007485479116439819\n",
      "0.011789675801992416\n",
      "0.007445333991199732\n",
      "0.005262252874672413\n",
      "0.005956415086984634\n",
      "0.00494958134368062\n",
      "0.00481716264039278\n",
      "0.006596044637262821\n",
      "0.003811054863035679\n",
      "0.007275494281202555\n",
      "0.006097841076552868\n",
      "0.007876362651586533\n",
      "0.005998562090098858\n",
      "0.0038811543490737677\n",
      "0.004360297229140997\n",
      "0.008132867515087128\n",
      "0.004756729584187269\n",
      "Epoch 12 train loss: 0.004756729584187269, val loss: 0.006738023832440376\n",
      "0.009206248447299004\n",
      "0.00658948440104723\n",
      "0.0062871971167624\n",
      "0.003383045783266425\n",
      "0.004542405251413584\n",
      "0.006362502928823233\n",
      "0.009326287545263767\n",
      "0.007101863157004118\n",
      "0.006067593581974506\n",
      "0.0058768452145159245\n",
      "0.007137513719499111\n",
      "0.004651564173400402\n",
      "0.006335115525871515\n",
      "0.005951127968728542\n",
      "0.009625893086194992\n",
      "0.007206394337117672\n",
      "0.01150704175233841\n",
      "0.007292254362255335\n",
      "0.00513035710901022\n",
      "0.0056423707865178585\n",
      "0.004814037587493658\n",
      "0.004930887836962938\n",
      "0.007193802390247583\n",
      "0.003919936716556549\n",
      "0.007448560558259487\n",
      "0.006310093216598034\n",
      "0.0076439594849944115\n",
      "0.00574779137969017\n",
      "0.004109541419893503\n",
      "0.004200451076030731\n",
      "0.00775379640981555\n",
      "0.004608236253261566\n",
      "Epoch 13 train loss: 0.004608236253261566, val loss: 0.007037335075438023\n",
      "0.008722339756786823\n",
      "0.006277088541537523\n",
      "0.00620347261428833\n",
      "0.00331508694216609\n",
      "0.004397993441671133\n",
      "0.006460064090788364\n",
      "0.009055033326148987\n",
      "0.007092341780662537\n",
      "0.005949562881141901\n",
      "0.0057532452046871185\n",
      "0.007039874792098999\n",
      "0.0042704688385128975\n",
      "0.0062904274091124535\n",
      "0.006101124454289675\n",
      "0.009254085831344128\n",
      "0.007081618998199701\n",
      "0.011361265555024147\n",
      "0.0073399594984948635\n",
      "0.005016373936086893\n",
      "0.005648032762110233\n",
      "0.00486834067851305\n",
      "0.0051900465041399\n",
      "0.007164536044001579\n",
      "0.0040739672258496284\n",
      "0.007533335126936436\n",
      "0.006135120056569576\n",
      "0.007780061569064856\n",
      "0.005590310785919428\n",
      "0.0038444576784968376\n",
      "0.004132573492825031\n",
      "0.007764490786939859\n",
      "0.004465334117412567\n",
      "Epoch 14 train loss: 0.004465334117412567, val loss: 0.007280389312654734\n",
      "0.008669793605804443\n",
      "0.006172391586005688\n",
      "0.006138320546597242\n",
      "0.0033369676675647497\n",
      "0.004341384395956993\n",
      "0.0058437976986169815\n",
      "0.008977462537586689\n",
      "0.007008030079305172\n",
      "0.005829697474837303\n",
      "0.005687740631401539\n",
      "0.006918954662978649\n",
      "0.004323783330619335\n",
      "0.006016605533659458\n",
      "0.0057209087535738945\n",
      "0.009615103714168072\n",
      "0.007615956943482161\n",
      "0.011781364679336548\n",
      "0.007016507908701897\n",
      "0.004942615516483784\n",
      "0.005417927633970976\n",
      "0.004669387359172106\n",
      "0.00495088892057538\n",
      "0.008001212030649185\n",
      "0.00408013304695487\n",
      "0.007614921312779188\n",
      "0.006367029156535864\n",
      "0.007631979882717133\n",
      "0.005183300003409386\n",
      "0.004076195415109396\n",
      "0.004273103550076485\n",
      "0.007651882246136665\n",
      "0.004320390988141298\n",
      "Epoch 15 train loss: 0.004320390988141298, val loss: 0.007167734205722809\n",
      "0.008179851807653904\n",
      "0.005732903257012367\n",
      "0.006061381660401821\n",
      "0.003340554889291525\n",
      "0.004744109697639942\n",
      "0.006695510819554329\n",
      "0.008820087648928165\n",
      "0.006925757974386215\n",
      "0.005613152403384447\n",
      "0.005501720122992992\n",
      "0.006916167680174112\n",
      "0.0038552142214030027\n",
      "0.006069895811378956\n",
      "0.006077222526073456\n",
      "0.009270569309592247\n",
      "0.0068299295380711555\n",
      "0.011066489852964878\n",
      "0.0074569485150277615\n",
      "0.004818109795451164\n",
      "0.004929514601826668\n",
      "0.004715151619166136\n",
      "0.004639712627977133\n",
      "0.007490084040910006\n",
      "0.0037308125756680965\n",
      "0.007061813026666641\n",
      "0.006150930654257536\n",
      "0.007260475307703018\n",
      "0.005190305411815643\n",
      "0.0037745574954897165\n",
      "0.0038725496269762516\n",
      "0.007690830156207085\n",
      "0.004230199847370386\n",
      "Epoch 16 train loss: 0.004230199847370386, val loss: 0.007508082780987024\n",
      "0.007924414239823818\n",
      "0.00570008996874094\n",
      "0.005834088660776615\n",
      "0.0033152110408991575\n",
      "0.004362910985946655\n",
      "0.006216541398316622\n",
      "0.008610311895608902\n",
      "0.0070813922211527824\n",
      "0.005891280248761177\n",
      "0.005663195624947548\n",
      "0.007271910551935434\n",
      "0.003894986817613244\n",
      "0.0058724055998027325\n",
      "0.005853121168911457\n",
      "0.009458377957344055\n",
      "0.007094325497746468\n",
      "0.01126216258853674\n",
      "0.007196443621069193\n",
      "0.004814591724425554\n",
      "0.005210693925619125\n",
      "0.0048098377883434296\n",
      "0.004894992336630821\n",
      "0.007520987652242184\n",
      "0.00412297248840332\n",
      "0.007029000204056501\n",
      "0.006298854481428862\n",
      "0.0072820745408535\n",
      "0.005252331495285034\n",
      "0.0038684445898979902\n",
      "0.00419275276362896\n",
      "0.00749487429857254\n",
      "0.004336300306022167\n",
      "Epoch 17 train loss: 0.004336300306022167, val loss: 0.007944243960082531\n",
      "0.008312010206282139\n",
      "0.005489172413945198\n",
      "0.005834496580064297\n",
      "0.0034814574755728245\n",
      "0.005005812272429466\n",
      "0.0069907731376588345\n",
      "0.008880356326699257\n",
      "0.007534429896622896\n",
      "0.006369641050696373\n",
      "0.006031051278114319\n",
      "0.007496534381061792\n",
      "0.00395108200609684\n",
      "0.0061019416898489\n",
      "0.006492419634014368\n",
      "0.009755603969097137\n",
      "0.0076206340454518795\n",
      "0.011313524097204208\n",
      "0.007459201384335756\n",
      "0.004838238470256329\n",
      "0.005247060675173998\n",
      "0.005007277708500624\n",
      "0.00519603630527854\n",
      "0.00812773872166872\n",
      "0.004408059176057577\n",
      "0.007398949004709721\n",
      "0.006618890445679426\n",
      "0.007359922863543034\n",
      "0.005375164560973644\n",
      "0.0038829410914331675\n",
      "0.004381921142339706\n",
      "0.007677916903048754\n",
      "0.004384929779917002\n",
      "Epoch 18 train loss: 0.004384929779917002, val loss: 0.00811432208865881\n",
      "0.008372007869184017\n",
      "0.005321858450770378\n",
      "0.005717163439840078\n",
      "0.003449160372838378\n",
      "0.004959967918694019\n",
      "0.007035574410110712\n",
      "0.009091880172491074\n",
      "0.007827156223356724\n",
      "0.006539412774145603\n",
      "0.006076426710933447\n",
      "0.008242427371442318\n",
      "0.0038742339238524437\n",
      "0.005667222198098898\n",
      "0.006700986530631781\n",
      "0.010396601632237434\n",
      "0.00783422589302063\n",
      "0.011944291181862354\n",
      "0.0076623642817139626\n",
      "0.004677460994571447\n",
      "0.0050806584767997265\n",
      "0.004891378805041313\n",
      "0.005878017749637365\n",
      "0.008979396894574165\n",
      "0.004916126374155283\n",
      "0.00806150957942009\n",
      "0.0072733573615550995\n",
      "0.007414064835757017\n",
      "0.0048040952533483505\n",
      "0.004924483131617308\n",
      "0.004688354209065437\n",
      "0.008271264843642712\n",
      "0.004941158462315798\n",
      "Epoch 19 train loss: 0.004941158462315798, val loss: 0.009683292359113693\n",
      "0.009637038223445415\n",
      "0.005631029140204191\n",
      "0.006259344983845949\n",
      "0.0034728937316685915\n",
      "0.005091237835586071\n",
      "0.007074389141052961\n",
      "0.009568593464791775\n",
      "0.008275027386844158\n",
      "0.007041032426059246\n",
      "0.0063925497233867645\n",
      "0.007864679209887981\n",
      "0.003936035092920065\n",
      "0.005162561312317848\n",
      "0.006304834969341755\n",
      "0.010206936858594418\n",
      "0.008025151677429676\n",
      "0.01224597543478012\n",
      "0.0077658286318182945\n",
      "0.004830039106309414\n",
      "0.005250509828329086\n",
      "0.0047426968812942505\n",
      "0.005200576037168503\n",
      "0.009182171896100044\n",
      "0.005612981040030718\n",
      "0.008041689172387123\n",
      "0.008210210129618645\n",
      "0.008206703700125217\n",
      "0.005652107298374176\n",
      "0.0045283762738108635\n",
      "0.004826019983738661\n",
      "0.007658360991626978\n",
      "0.004731512162834406\n",
      "Epoch 20 train loss: 0.004731512162834406, val loss: 0.009968245401978493\n",
      "0.01069676410406828\n",
      "0.006624006666243076\n",
      "0.0068773506209254265\n",
      "0.003850644687190652\n",
      "0.004326343536376953\n",
      "0.006885388866066933\n",
      "0.00973821897059679\n",
      "0.008734939619898796\n",
      "0.00825338065624237\n",
      "0.007661973591893911\n",
      "0.009899831376969814\n",
      "0.004815617110580206\n",
      "0.005010189954191446\n",
      "0.006742821075022221\n",
      "0.01009798888117075\n",
      "0.008341500535607338\n",
      "0.013038252480328083\n",
      "0.009793170727789402\n",
      "0.006136770825833082\n",
      "0.006024941336363554\n",
      "0.0052400995045900345\n",
      "0.005139903165400028\n",
      "0.009616104885935783\n",
      "0.00534369982779026\n",
      "0.008330848067998886\n",
      "0.00963597558438778\n",
      "0.008798516355454922\n",
      "0.005908214021474123\n",
      "0.006080077029764652\n",
      "0.004230550490319729\n",
      "0.007175613660365343\n",
      "0.00448310561478138\n",
      "Epoch 21 train loss: 0.00448310561478138, val loss: 0.011368936859071255\n",
      "0.011350304819643497\n",
      "0.0068328664638102055\n",
      "0.008180610835552216\n",
      "0.0045935166999697685\n",
      "0.00437540328130126\n",
      "0.00767306424677372\n",
      "0.009144416078925133\n",
      "0.007910607382655144\n",
      "0.007675624452531338\n",
      "0.008351909928023815\n",
      "0.010366218164563179\n",
      "0.005599872209131718\n",
      "0.005735956132411957\n",
      "0.007223912980407476\n",
      "0.008838359266519547\n",
      "0.007475785445421934\n",
      "0.013156776316463947\n",
      "0.009853718802332878\n",
      "0.0061116451397538185\n",
      "0.0067866891622543335\n",
      "0.005972434300929308\n",
      "0.004389653913676739\n",
      "0.007902116514742374\n",
      "0.004507515579462051\n",
      "0.006148227024823427\n",
      "0.007890641689300537\n",
      "0.008626636117696762\n",
      "0.007099145092070103\n",
      "0.005426693707704544\n",
      "0.003973245620727539\n",
      "0.006154617294669151\n",
      "0.003785948269069195\n",
      "Epoch 22 train loss: 0.003785948269069195, val loss: 0.009014043025672436\n",
      "0.009107932448387146\n",
      "0.006087424233555794\n",
      "0.008321905508637428\n",
      "0.005740462336689234\n",
      "0.004594565834850073\n",
      "0.009271450340747833\n",
      "0.009301399812102318\n",
      "0.007712433114647865\n",
      "0.007533313240855932\n",
      "0.008541803807020187\n",
      "0.012749021872878075\n",
      "0.006331318989396095\n",
      "0.0061704376712441444\n",
      "0.009431845508515835\n",
      "0.009770378470420837\n",
      "0.006837382446974516\n",
      "0.012830887921154499\n",
      "0.010705620050430298\n",
      "0.007030795328319073\n",
      "0.006775789428502321\n",
      "0.006929365918040276\n",
      "0.004732117056846619\n",
      "0.00926948431879282\n",
      "0.004258628934621811\n",
      "0.005846600979566574\n",
      "0.00758045818656683\n",
      "0.0072762854397296906\n",
      "0.005859291646629572\n",
      "0.0057604070752859116\n",
      "0.0038725649937987328\n",
      "0.006484110839664936\n",
      "0.0035100248642265797\n",
      "Epoch 23 train loss: 0.0035100248642265797, val loss: 0.008496393449604511\n",
      "0.009409070014953613\n",
      "0.005758766550570726\n",
      "0.008432156406342983\n",
      "0.005758536048233509\n",
      "0.004448150750249624\n",
      "0.007179443724453449\n",
      "0.010111462324857712\n",
      "0.007524793967604637\n",
      "0.006729607470333576\n",
      "0.006452647037804127\n",
      "0.010807959362864494\n",
      "0.0072488561272621155\n",
      "0.005922284908592701\n",
      "0.008390604518353939\n",
      "0.010448497720062733\n",
      "0.006662548054009676\n",
      "0.012750353664159775\n",
      "0.009637703187763691\n",
      "0.007173343561589718\n",
      "0.006795917637646198\n",
      "0.007021477445960045\n",
      "0.004231535829603672\n",
      "0.00899554044008255\n",
      "0.004608019720762968\n",
      "0.004703732673078775\n",
      "0.006076973397284746\n",
      "0.006573052145540714\n",
      "0.006002689246088266\n",
      "0.005295071750879288\n",
      "0.00403756694868207\n",
      "0.005957729648798704\n",
      "0.003832611721009016\n",
      "Epoch 24 train loss: 0.003832611721009016, val loss: 0.007414791267365217\n",
      "0.008193274028599262\n",
      "0.005494874436408281\n",
      "0.007508729584515095\n",
      "0.005749799311161041\n",
      "0.004807363264262676\n",
      "0.0071949223056435585\n",
      "0.009455054067075253\n",
      "0.008101899176836014\n",
      "0.006935679353773594\n",
      "0.006112788803875446\n",
      "0.01164541020989418\n",
      "0.007087101228535175\n",
      "0.0055993106216192245\n",
      "0.00781860202550888\n",
      "0.011512847617268562\n",
      "0.0073251668363809586\n",
      "0.011986983940005302\n",
      "0.008811315521597862\n",
      "0.006460655480623245\n",
      "0.006654172204434872\n",
      "0.007859144359827042\n",
      "0.0043319957330822945\n",
      "0.0077617890201509\n",
      "0.005565713159739971\n",
      "0.004223284311592579\n",
      "0.005198714789003134\n",
      "0.005609586834907532\n",
      "0.0052717202343046665\n",
      "0.004552194848656654\n",
      "0.003940170630812645\n",
      "0.006298378575593233\n",
      "0.003420109860599041\n",
      "Epoch 25 train loss: 0.003420109860599041, val loss: 0.007718607317656279\n",
      "0.00848292000591755\n",
      "0.00642332062125206\n",
      "0.005739361979067326\n",
      "0.004920057021081448\n",
      "0.005310562439262867\n",
      "0.0052724252454936504\n",
      "0.008972716517746449\n",
      "0.007834464311599731\n",
      "0.007523747626692057\n",
      "0.00439273938536644\n",
      "0.008102466352283955\n",
      "0.007240600418299437\n",
      "0.005862327758222818\n",
      "0.005987192038446665\n",
      "0.010848868638277054\n",
      "0.007481944747269154\n",
      "0.01141980942338705\n",
      "0.007006101310253143\n",
      "0.004993671551346779\n",
      "0.005374929867684841\n",
      "0.007561597041785717\n",
      "0.004789209458976984\n",
      "0.007019401527941227\n",
      "0.0052467272616922855\n",
      "0.003953926265239716\n",
      "0.004557284526526928\n",
      "0.0051611377857625484\n",
      "0.005434812977910042\n",
      "0.004131386987864971\n",
      "0.003878548974171281\n",
      "0.006698640529066324\n",
      "0.0033755027689039707\n",
      "Epoch 26 train loss: 0.0033755027689039707, val loss: 0.007591753266751766\n",
      "0.0080104423686862\n",
      "0.0072762444615364075\n",
      "0.004511686507612467\n",
      "0.0033322498202323914\n",
      "0.0046135783195495605\n",
      "0.005156436935067177\n",
      "0.008207874372601509\n",
      "0.007200667634606361\n",
      "0.007207680493593216\n",
      "0.003997613210231066\n",
      "0.006130507215857506\n",
      "0.005318567156791687\n",
      "0.005442628636956215\n",
      "0.004886286333203316\n",
      "0.010006657801568508\n",
      "0.0073101576417684555\n",
      "0.01152561791241169\n",
      "0.006339665036648512\n",
      "0.004692563321441412\n",
      "0.005116691347211599\n",
      "0.007695404812693596\n",
      "0.005273607559502125\n",
      "0.006968293804675341\n",
      "0.005799111444503069\n",
      "0.004510602913796902\n",
      "0.0044116852805018425\n",
      "0.005092585925012827\n",
      "0.004091025795787573\n",
      "0.004316139500588179\n",
      "0.0038421903736889362\n",
      "0.007303682621568441\n",
      "0.003856309922412038\n",
      "Epoch 27 train loss: 0.003856309922412038, val loss: 0.00811169482767582\n",
      "0.009003236889839172\n",
      "0.007752734702080488\n",
      "0.0046188710257411\n",
      "0.002914735348895192\n",
      "0.004433055873960257\n",
      "0.005813791416585445\n",
      "0.007993418723344803\n",
      "0.007687758654356003\n",
      "0.00699052307754755\n",
      "0.0039700474590063095\n",
      "0.005413622595369816\n",
      "0.004562768619507551\n",
      "0.005210487172007561\n",
      "0.004555216990411282\n",
      "0.009708134457468987\n",
      "0.007143356371670961\n",
      "0.011090711690485477\n",
      "0.0063782609067857265\n",
      "0.004749173764139414\n",
      "0.004407677333801985\n",
      "0.006527899764478207\n",
      "0.005039514973759651\n",
      "0.0064512332901358604\n",
      "0.005150680895894766\n",
      "0.004883403889834881\n",
      "0.0053173694759607315\n",
      "0.005479047540575266\n",
      "0.004359883721917868\n",
      "0.003192191245034337\n",
      "0.0030404841527342796\n",
      "0.006722277961671352\n",
      "0.0036596653517335653\n",
      "Epoch 28 train loss: 0.0036596653517335653, val loss: 0.007512444630265236\n",
      "0.007681101560592651\n",
      "0.005987555719912052\n",
      "0.004215475171804428\n",
      "0.0031292936764657497\n",
      "0.003258834360167384\n",
      "0.005232223309576511\n",
      "0.007075931876897812\n",
      "0.007838446646928787\n",
      "0.006574450992047787\n",
      "0.004349163733422756\n",
      "0.0062240902334451675\n",
      "0.003864545840770006\n",
      "0.004810140002518892\n",
      "0.004566121380776167\n",
      "0.009791197255253792\n",
      "0.007209313102066517\n",
      "0.010743780992925167\n",
      "0.006481070537120104\n",
      "0.004690696019679308\n",
      "0.004449340980499983\n",
      "0.006254907231777906\n",
      "0.0042012520134449005\n",
      "0.006828921847045422\n",
      "0.0058912173844873905\n",
      "0.00488736666738987\n",
      "0.004802126437425613\n",
      "0.005671761929988861\n",
      "0.004248627461493015\n",
      "0.0037571643479168415\n",
      "0.003325103782117367\n",
      "0.006279326975345612\n",
      "0.00369583279825747\n",
      "Epoch 29 train loss: 0.00369583279825747, val loss: 0.008220252580940723\n",
      "0.008677339181303978\n",
      "0.007265035063028336\n",
      "0.004855970852077007\n",
      "0.0028216862119734287\n",
      "0.004505482502281666\n",
      "0.00569402938708663\n",
      "0.007484637200832367\n",
      "0.008436117321252823\n",
      "0.007170939352363348\n",
      "0.004353439901024103\n",
      "0.005935485474765301\n",
      "0.004399131052196026\n",
      "0.005136475898325443\n",
      "0.00392195675522089\n",
      "0.009310719557106495\n",
      "0.007150752004235983\n",
      "0.010932479053735733\n",
      "0.006718694232404232\n",
      "0.00464114174246788\n",
      "0.004458039999008179\n",
      "0.006343889515846968\n",
      "0.004134868271648884\n",
      "0.006014368962496519\n",
      "0.005263608414679766\n",
      "0.005504915025085211\n",
      "0.005472451914101839\n",
      "0.006019811145961285\n",
      "0.004518244415521622\n",
      "0.003630097024142742\n",
      "0.0035487867426127195\n",
      "0.005471409298479557\n",
      "0.0032052290625870228\n",
      "Epoch 30 train loss: 0.0032052290625870228, val loss: 0.008676029741764069\n",
      "0.0088891526684165\n",
      "0.007381706032902002\n",
      "0.005335479974746704\n",
      "0.003750649280846119\n",
      "0.004208538215607405\n",
      "0.005738093517720699\n",
      "0.007114664185792208\n",
      "0.008469429798424244\n",
      "0.007154090330004692\n",
      "0.004669172689318657\n",
      "0.006320741027593613\n",
      "0.004796717781573534\n",
      "0.005292351357638836\n",
      "0.004560107830911875\n",
      "0.00985738355666399\n",
      "0.007055419031530619\n",
      "0.010825805366039276\n",
      "0.006896372884511948\n",
      "0.0044469814747571945\n",
      "0.004944080486893654\n",
      "0.007437462918460369\n",
      "0.004182915203273296\n",
      "0.005794077180325985\n",
      "0.005974292289465666\n",
      "0.005713148042559624\n",
      "0.005309767555445433\n",
      "0.005957098677754402\n",
      "0.004950238857418299\n",
      "0.004447714425623417\n",
      "0.004201917443424463\n",
      "0.005514008924365044\n",
      "0.0031584855169057846\n",
      "Epoch 31 train loss: 0.0031584855169057846, val loss: 0.008814111351966858\n",
      "0.009127429686486721\n",
      "0.007385633885860443\n",
      "0.005578349344432354\n",
      "0.004375727381557226\n",
      "0.005185957066714764\n",
      "0.005822368897497654\n",
      "0.007473369129002094\n",
      "0.00854667741805315\n",
      "0.0074527813121676445\n",
      "0.00447767972946167\n",
      "0.006104735657572746\n",
      "0.004876163322478533\n",
      "0.005898292642086744\n",
      "0.004741104785352945\n",
      "0.009557732380926609\n",
      "0.0074863433837890625\n",
      "0.0108773373067379\n",
      "0.0067224991507828236\n",
      "0.003969419747591019\n",
      "0.004614238627254963\n",
      "0.007952501066029072\n",
      "0.004554606042802334\n",
      "0.005071029532700777\n",
      "0.005731180775910616\n",
      "0.005981113761663437\n",
      "0.005094853229820728\n",
      "0.005161535460501909\n",
      "0.004631188698112965\n",
      "0.004371952265501022\n",
      "0.004794098436832428\n",
      "0.005590321496129036\n",
      "0.0034860556479543447\n",
      "Epoch 32 train loss: 0.0034860556479543447, val loss: 0.00821871217340231\n",
      "0.008551170118153095\n",
      "0.006946291774511337\n",
      "0.005283944308757782\n",
      "0.003930564504116774\n",
      "0.005004783160984516\n",
      "0.006073242053389549\n",
      "0.007753930985927582\n",
      "0.008112272247672081\n",
      "0.006928383372724056\n",
      "0.004267681855708361\n",
      "0.00562114454805851\n",
      "0.004127230029553175\n",
      "0.005280752200633287\n",
      "0.004811531864106655\n",
      "0.01035503949970007\n",
      "0.007217670790851116\n",
      "0.010959381237626076\n",
      "0.006198396906256676\n",
      "0.003852194407954812\n",
      "0.004490349907428026\n",
      "0.00861133448779583\n",
      "0.004858117550611496\n",
      "0.005100612062960863\n",
      "0.005991623271256685\n",
      "0.006392029579728842\n",
      "0.005137490574270487\n",
      "0.004861046094447374\n",
      "0.00415925495326519\n",
      "0.004828156437724829\n",
      "0.005714884493499994\n",
      "0.006436572410166264\n",
      "0.003766438690945506\n",
      "Epoch 33 train loss: 0.003766438690945506, val loss: 0.00867278128862381\n",
      "0.009272311814129353\n",
      "0.007243653759360313\n",
      "0.005595369264483452\n",
      "0.0031260051764547825\n",
      "0.004882408306002617\n",
      "0.006294588092714548\n",
      "0.00842243991792202\n",
      "0.008013114333152771\n",
      "0.006809843238443136\n",
      "0.004098761826753616\n",
      "0.0048482296988368034\n",
      "0.00403411453589797\n",
      "0.005393367260694504\n",
      "0.004763237666338682\n",
      "0.01060535479336977\n",
      "0.006919217295944691\n",
      "0.010586574673652649\n",
      "0.00543892290443182\n",
      "0.0041193668730556965\n",
      "0.004056706093251705\n",
      "0.007277118973433971\n",
      "0.004778657574206591\n",
      "0.0051414757035672665\n",
      "0.004931777250021696\n",
      "0.005750656593590975\n",
      "0.004744159057736397\n",
      "0.0049547357484698296\n",
      "0.0037841061130166054\n",
      "0.0041128844022750854\n",
      "0.005361342802643776\n",
      "0.006608482450246811\n",
      "0.003802735125645995\n",
      "Epoch 34 train loss: 0.003802735125645995, val loss: 0.00877357181161642\n",
      "0.008637633174657822\n",
      "0.006635998375713825\n",
      "0.005536429118365049\n",
      "0.0026482015382498503\n",
      "0.004098999314010143\n",
      "0.006042920518666506\n",
      "0.008622976019978523\n",
      "0.007841081358492374\n",
      "0.006409648340195417\n",
      "0.003973533399403095\n",
      "0.004300208762288094\n",
      "0.003715924918651581\n",
      "0.005022276192903519\n",
      "0.004454566165804863\n",
      "0.010713664814829826\n",
      "0.0073450468480587006\n",
      "0.009935651905834675\n",
      "0.0053805578500032425\n",
      "0.004160381853580475\n",
      "0.0038746995851397514\n",
      "0.0060648005455732346\n",
      "0.00419500470161438\n",
      "0.005216086748987436\n",
      "0.004993373993784189\n",
      "0.005575088784098625\n",
      "0.0048915003426373005\n",
      "0.005183082073926926\n",
      "0.003506454173475504\n",
      "0.0037724736612290144\n",
      "0.00477240327745676\n",
      "0.0065808589570224285\n",
      "0.004467491060495377\n",
      "Epoch 35 train loss: 0.004467491060495377, val loss: 0.008641240186989307\n",
      "0.009370531886816025\n",
      "0.006207498721778393\n",
      "0.00562245212495327\n",
      "0.0024945151526480913\n",
      "0.004024488851428032\n",
      "0.006020981352776289\n",
      "0.00873258151113987\n",
      "0.008233239874243736\n",
      "0.00666422676295042\n",
      "0.004142292309552431\n",
      "0.004266553558409214\n",
      "0.003540776902809739\n",
      "0.005389275029301643\n",
      "0.004499509464949369\n",
      "0.010681122541427612\n",
      "0.0074571892619132996\n",
      "0.009498508647084236\n",
      "0.005278205033391714\n",
      "0.004167554900050163\n",
      "0.003862815210595727\n",
      "0.004885479342192411\n",
      "0.003949039615690708\n",
      "0.005136805586516857\n",
      "0.004564395174384117\n",
      "0.005206176079809666\n",
      "0.004801447968930006\n",
      "0.005619416479021311\n",
      "0.0031060571782290936\n",
      "0.0035063934046775103\n",
      "0.004560301546007395\n",
      "0.006711786612868309\n",
      "0.004142811521887779\n",
      "Epoch 36 train loss: 0.004142811521887779, val loss: 0.009024836122989655\n",
      "0.00974795501679182\n",
      "0.006173230707645416\n",
      "0.00621653301641345\n",
      "0.0026689600199460983\n",
      "0.00395711325109005\n",
      "0.005888580810278654\n",
      "0.008997974917292595\n",
      "0.008743931539356709\n",
      "0.006935593206435442\n",
      "0.004448368679732084\n",
      "0.004339146427810192\n",
      "0.0037223147228360176\n",
      "0.005108829587697983\n",
      "0.004630104638636112\n",
      "0.010709472000598907\n",
      "0.007859909906983376\n",
      "0.009047259576618671\n",
      "0.005304594058543444\n",
      "0.004146300256252289\n",
      "0.0038812924176454544\n",
      "0.0040964120998978615\n",
      "0.0036882469430565834\n",
      "0.005452735815197229\n",
      "0.005054962821304798\n",
      "0.005597486160695553\n",
      "0.005230804905295372\n",
      "0.006118365563452244\n",
      "0.0029648272320628166\n",
      "0.0031311644706875086\n",
      "0.0042214165441691875\n",
      "0.0066882288083434105\n",
      "0.00426847068592906\n",
      "Epoch 37 train loss: 0.00426847068592906, val loss: 0.009361028671264648\n",
      "0.010257367044687271\n",
      "0.006000007502734661\n",
      "0.0064009311608970165\n",
      "0.0027735442854464054\n",
      "0.004059943370521069\n",
      "0.005925212986767292\n",
      "0.009219014085829258\n",
      "0.009495328180491924\n",
      "0.007538404315710068\n",
      "0.005068586207926273\n",
      "0.004649792797863483\n",
      "0.0039133732207119465\n",
      "0.005547632928937674\n",
      "0.0051406691782176495\n",
      "0.01044610794633627\n",
      "0.008742976933717728\n",
      "0.009216000325977802\n",
      "0.006173803936690092\n",
      "0.004548113793134689\n",
      "0.004263248760253191\n",
      "0.0041178809478878975\n",
      "0.0036441930569708347\n",
      "0.00557899521663785\n",
      "0.005195226054638624\n",
      "0.006040909327566624\n",
      "0.005914718843996525\n",
      "0.007269465364515781\n",
      "0.0034333374351263046\n",
      "0.003045485122129321\n",
      "0.0040929582901299\n",
      "0.006982091814279556\n",
      "0.004390030167996883\n",
      "Epoch 38 train loss: 0.004390030167996883, val loss: 0.010371091775596142\n",
      "0.011632352136075497\n",
      "0.007302701473236084\n",
      "0.007125442381948233\n",
      "0.003159414976835251\n",
      "0.003719284664839506\n",
      "0.0057370541617274284\n",
      "0.009256318211555481\n",
      "0.01024241279810667\n",
      "0.008300221525132656\n",
      "0.005894521716982126\n",
      "0.005509937647730112\n",
      "0.004007229581475258\n",
      "0.005957277957350016\n",
      "0.006055004894733429\n",
      "0.010440457612276077\n",
      "0.009237769991159439\n",
      "0.009778079576790333\n",
      "0.007308118510991335\n",
      "0.005154195241630077\n",
      "0.004831534810364246\n",
      "0.004294322803616524\n",
      "0.0035638855770230293\n",
      "0.005310556851327419\n",
      "0.005480632651597261\n",
      "0.006264002062380314\n",
      "0.006682875566184521\n",
      "0.007638328708708286\n",
      "0.0035916136112064123\n",
      "0.0031145173124969006\n",
      "0.003337559290230274\n",
      "0.006019824184477329\n",
      "0.004080853890627623\n",
      "Epoch 39 train loss: 0.004080853890627623, val loss: 0.010589661076664925\n",
      "0.011147537268698215\n",
      "0.007664876990020275\n",
      "0.008427907712757587\n",
      "0.004720795899629593\n",
      "0.003367756260558963\n",
      "0.005670744460076094\n",
      "0.008887210860848427\n",
      "0.010184425860643387\n",
      "0.009363238699734211\n",
      "0.008255915716290474\n",
      "0.008143196813762188\n",
      "0.004209396429359913\n",
      "0.006043797358870506\n",
      "0.00658333720639348\n",
      "0.00898844562470913\n",
      "0.008789400570094585\n",
      "0.010580291971564293\n",
      "0.0090939961373806\n",
      "0.006723108701407909\n",
      "0.006296468898653984\n",
      "0.005250453017652035\n",
      "0.0035771895200014114\n",
      "0.005450372118502855\n",
      "0.006361594423651695\n",
      "0.0066642253659665585\n",
      "0.007858917117118835\n",
      "0.008892727084457874\n",
      "0.004355290904641151\n",
      "0.004063038621097803\n",
      "0.0039430842734873295\n",
      "0.006124346517026424\n",
      "0.003897507209330797\n",
      "Epoch 40 train loss: 0.003897507209330797, val loss: 0.0110549945384264\n",
      "0.01094522699713707\n",
      "0.00828344002366066\n",
      "0.01094542071223259\n",
      "0.005897120106965303\n",
      "0.0030804937705397606\n",
      "0.005310641136020422\n",
      "0.008984291926026344\n",
      "0.008911076933145523\n",
      "0.00894692912697792\n",
      "0.009461252018809319\n",
      "0.010771269910037518\n",
      "0.005386357195675373\n",
      "0.0054476396180689335\n",
      "0.007906590588390827\n",
      "0.008005624637007713\n",
      "0.00798504427075386\n",
      "0.0105142155662179\n",
      "0.008990553207695484\n",
      "0.007245203480124474\n",
      "0.007329713087528944\n",
      "0.006892604753375053\n",
      "0.0032599857077002525\n",
      "0.005324502941220999\n",
      "0.00575528247281909\n",
      "0.005548030138015747\n",
      "0.006900350097566843\n",
      "0.008797457441687584\n",
      "0.00590444216504693\n",
      "0.004414039198309183\n",
      "0.004674574360251427\n",
      "0.005698789842426777\n",
      "0.0040162717923521996\n",
      "Epoch 41 train loss: 0.0040162717923521996, val loss: 0.008913067169487476\n",
      "0.008419796824455261\n",
      "0.006549064069986343\n",
      "0.010145382955670357\n",
      "0.007928856648504734\n",
      "0.004453652538359165\n",
      "0.004478782881051302\n",
      "0.008657939732074738\n",
      "0.007313814479857683\n",
      "0.007866891101002693\n",
      "0.008139909245073795\n",
      "0.012928648851811886\n",
      "0.007196442224085331\n",
      "0.005311862099915743\n",
      "0.005819133948534727\n",
      "0.008697250857949257\n",
      "0.008180825971066952\n",
      "0.01177546102553606\n",
      "0.006773360073566437\n",
      "0.0059133004397153854\n",
      "0.006912977434694767\n",
      "0.009859830141067505\n",
      "0.0037481444887816906\n",
      "0.004010747652500868\n",
      "0.0065316613763570786\n",
      "0.005293260328471661\n",
      "0.005408663302659988\n",
      "0.006164140067994595\n",
      "0.005188849754631519\n",
      "0.005262224934995174\n",
      "0.005663643125444651\n",
      "0.007017446216195822\n",
      "0.0036498766858130693\n",
      "Epoch 42 train loss: 0.0036498766858130693, val loss: 0.009291395545005798\n",
      "0.008692603558301926\n",
      "0.010148719884455204\n",
      "0.006858370266854763\n",
      "0.00486021488904953\n",
      "0.0045933895744383335\n",
      "0.003160082036629319\n",
      "0.007020357996225357\n",
      "0.005605956073850393\n",
      "0.006096182856708765\n",
      "0.006913176737725735\n",
      "0.006309465505182743\n",
      "0.005223169922828674\n",
      "0.005993131082504988\n",
      "0.005309970583766699\n",
      "0.006883877329528332\n",
      "0.006551866419613361\n",
      "0.010989868082106113\n",
      "0.005699165631085634\n",
      "0.005295103415846825\n",
      "0.004248855635523796\n",
      "0.007879668846726418\n",
      "0.004504597745835781\n",
      "0.0037643974646925926\n",
      "0.004028072115033865\n",
      "0.005286112427711487\n",
      "0.005843060556799173\n",
      "0.007201142143458128\n",
      "0.0036233295686542988\n",
      "0.004477441310882568\n",
      "0.0058959173038601875\n",
      "0.009365420788526535\n",
      "0.006329147145152092\n",
      "Epoch 43 train loss: 0.006329147145152092, val loss: 0.008551775477826595\n",
      "0.00713370181620121\n",
      "0.010373813100159168\n",
      "0.007517384365200996\n",
      "0.003686461830511689\n",
      "0.0023309180978685617\n",
      "0.003520029131323099\n",
      "0.007661458570510149\n",
      "0.00636825617402792\n",
      "0.005537921562790871\n",
      "0.004642624873667955\n",
      "0.00575332622975111\n",
      "0.004690292291343212\n",
      "0.0051741767674684525\n",
      "0.004384960513561964\n",
      "0.006240690127015114\n",
      "0.0061487555503845215\n",
      "0.009083216078579426\n",
      "0.004828836303204298\n",
      "0.005060337949544191\n",
      "0.004372269846498966\n",
      "0.004093996249139309\n",
      "0.002957393880933523\n",
      "0.0031770330388098955\n",
      "0.004380475264042616\n",
      "0.004257421940565109\n",
      "0.004357615951448679\n",
      "0.0071008820086717606\n",
      "0.0032029596623033285\n",
      "0.0028038560412824154\n",
      "0.0037470238748937845\n",
      "0.008561856113374233\n",
      "0.005986690055578947\n",
      "Epoch 44 train loss: 0.005986690055578947, val loss: 0.008048777468502522\n",
      "0.006183939054608345\n",
      "0.006770046427845955\n",
      "0.007424002047628164\n",
      "0.005845469422638416\n",
      "0.0029037431813776493\n",
      "0.002927357330918312\n",
      "0.0072559271939098835\n",
      "0.007699913345277309\n",
      "0.0076202466152608395\n",
      "0.005554018542170525\n",
      "0.005190508905798197\n",
      "0.005547993816435337\n",
      "0.006668586749583483\n",
      "0.005179781466722488\n",
      "0.00534670427441597\n",
      "0.005871468223631382\n",
      "0.009642066434025764\n",
      "0.006068073678761721\n",
      "0.005935505963861942\n",
      "0.005353596992790699\n",
      "0.0036836224608123302\n",
      "0.002727365121245384\n",
      "0.0032515698112547398\n",
      "0.003939468413591385\n",
      "0.004447836894541979\n",
      "0.004916585981845856\n",
      "0.007005315739661455\n",
      "0.004545978736132383\n",
      "0.0035379761829972267\n",
      "0.0036237698514014482\n",
      "0.006076521705836058\n",
      "0.0037860386073589325\n",
      "Epoch 45 train loss: 0.0037860386073589325, val loss: 0.007989717647433281\n",
      "0.005841327365487814\n",
      "0.005387279205024242\n",
      "0.00704472791403532\n",
      "0.005218604579567909\n",
      "0.003692205063998699\n",
      "0.004280509892851114\n",
      "0.005459440406411886\n",
      "0.006251474842429161\n",
      "0.006468100938946009\n",
      "0.00714826537296176\n",
      "0.007009235676378012\n",
      "0.006168254651129246\n",
      "0.005175058264285326\n",
      "0.004875089041888714\n",
      "0.0057357000187039375\n",
      "0.005315798334777355\n",
      "0.009178770706057549\n",
      "0.006956035271286964\n",
      "0.006056088954210281\n",
      "0.004887528717517853\n",
      "0.004334883764386177\n",
      "0.0026786448433995247\n",
      "0.0034484940115362406\n",
      "0.0035807660315185785\n",
      "0.003902959171682596\n",
      "0.004659248515963554\n",
      "0.005986536853015423\n",
      "0.003396784421056509\n",
      "0.0030640431214123964\n",
      "0.003482985543087125\n",
      "0.005561497528105974\n",
      "0.0034358559641987085\n",
      "Epoch 46 train loss: 0.0034358559641987085, val loss: 0.006704259198158979\n",
      "0.005168931558728218\n",
      "0.004993346054106951\n",
      "0.00747643830254674\n",
      "0.004203656688332558\n",
      "0.0030417651869356632\n",
      "0.0036595643032342196\n",
      "0.005093671381473541\n",
      "0.005194481462240219\n",
      "0.005417115520685911\n",
      "0.005689023528248072\n",
      "0.00621076999232173\n",
      "0.004017479717731476\n",
      "0.00534986425191164\n",
      "0.005318631883710623\n",
      "0.005238623358309269\n",
      "0.004456251859664917\n",
      "0.008735418319702148\n",
      "0.006129133049398661\n",
      "0.006290119141340256\n",
      "0.004626334644854069\n",
      "0.0038650527130812407\n",
      "0.002815133659169078\n",
      "0.00372999906539917\n",
      "0.003002278972417116\n",
      "0.0035759438760578632\n",
      "0.003960342146456242\n",
      "0.00571465864777565\n",
      "0.0032192817889153957\n",
      "0.0027797336224466562\n",
      "0.003937987610697746\n",
      "0.00556554039940238\n",
      "0.0028142565861344337\n",
      "Epoch 47 train loss: 0.0028142565861344337, val loss: 0.006010125856846571\n",
      "0.004973658826202154\n",
      "0.0045990352518856525\n",
      "0.0063168019987642765\n",
      "0.0043411231599748135\n",
      "0.002724079182371497\n",
      "0.0027014215011149645\n",
      "0.004732763394713402\n",
      "0.004519807640463114\n",
      "0.004291229881346226\n",
      "0.004829940851777792\n",
      "0.00593596650287509\n",
      "0.004427164793014526\n",
      "0.004692547954618931\n",
      "0.00428721122443676\n",
      "0.005024081561714411\n",
      "0.00409749336540699\n",
      "0.008830601349473\n",
      "0.004852598998695612\n",
      "0.005414139479398727\n",
      "0.0038882337976247072\n",
      "0.0045416164211928844\n",
      "0.0025794184766709805\n",
      "0.0031604303512722254\n",
      "0.0028629680164158344\n",
      "0.0037887138314545155\n",
      "0.0036767018027603626\n",
      "0.00540884118527174\n",
      "0.0031719631515443325\n",
      "0.003040432697162032\n",
      "0.0048454152420163155\n",
      "0.006451018154621124\n",
      "0.003418107284232974\n",
      "Epoch 48 train loss: 0.003418107284232974, val loss: 0.006018426734954119\n",
      "0.005489082541316748\n",
      "0.007069804705679417\n",
      "0.005944675765931606\n",
      "0.0036181604955345392\n",
      "0.00269710854627192\n",
      "0.002473706379532814\n",
      "0.0049872929230332375\n",
      "0.004215257242321968\n",
      "0.004071429371833801\n",
      "0.004085179418325424\n",
      "0.004320234060287476\n",
      "0.004311048425734043\n",
      "0.004765316843986511\n",
      "0.0038095861673355103\n",
      "0.004274830222129822\n",
      "0.0038223532028496265\n",
      "0.008306285366415977\n",
      "0.004235586151480675\n",
      "0.004570123739540577\n",
      "0.0032580187544226646\n",
      "0.003845615778118372\n",
      "0.0028495348524302244\n",
      "0.002968984190374613\n",
      "0.0026685113552957773\n",
      "0.003303220495581627\n",
      "0.0037032626569271088\n",
      "0.005746001377701759\n",
      "0.003913408145308495\n",
      "0.00247024092823267\n",
      "0.0039025936275720596\n",
      "0.006310614291578531\n",
      "0.0038277595303952694\n",
      "Epoch 49 train loss: 0.0038277595303952694, val loss: 0.005665640812367201\n",
      "tensor(0.0057, grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# attention_estimator = AttentionEstimator(Dinput, 1)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.AdamW(attention_estimator.parameters(), lr=0.001)\n",
    "\n",
    "# val_losses = []\n",
    "\n",
    "# for epoch in range(50):\n",
    "#     for batch in train_dataloader:\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         B, T, D = batch['obs'].shape\n",
    "#         nobs = normalizer['obs'].unnormalize(batch['obs'])\n",
    "        \n",
    "#         # Reshape input to B*T, D+1 where last dim is time index\n",
    "#         obs = torch.repeat_interleave(nobs[:, :2, :-1].reshape(B, -1), T, dim=0)  # B*T, 2D\n",
    "#         time_idx = torch.arange(T, device=obs.device).repeat(B).unsqueeze(-1)  # B*T, 1\n",
    "#         # obs = torch.cat([obs, time_idx.unsqueeze(-1)], dim=-1)  # B*T, D\n",
    "        \n",
    "#         # Reshape target to B*T, 1\n",
    "#         attention = nobs[:, :, -1].reshape(-1, 1)  # B*T, 1\n",
    "        \n",
    "#         output = attention_estimator(obs, time_idx)\n",
    "#         loss = criterion(output, attention)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print(loss.item())\n",
    "        \n",
    "#     for batch in val_dataloader:\n",
    "#         B, T, D = batch['obs'].shape\n",
    "#         nobs = normalizer['obs'].unnormalize(batch['obs'])\n",
    "        \n",
    "#         # Reshape input to B*T, D+1 where last dim is time index\n",
    "#         obs = torch.repeat_interleave(nobs[:, :2, :-1].reshape(B, -1), T, dim=0)  # B*T, 2D\n",
    "#         time_idx = torch.arange(T, device=obs.device).repeat(B).unsqueeze(-1)  # B*T, 1\n",
    "#         # obs = torch.cat([obs, time_idx.unsqueeze(-1)], dim=-1)  # B*T, D\n",
    "        \n",
    "#         # Reshape target to B*T, 1\n",
    "#         attention = nobs[:, :, -1].reshape(-1, 1)  # B*T, 1\n",
    "        \n",
    "#         output = attention_estimator(obs, time_idx)\n",
    "#         val_loss = criterion(output, attention)\n",
    "#         val_losses.append(val_loss.item())\n",
    "        \n",
    "#     print(f\"Epoch {epoch} train loss: {loss}, val loss: {val_losses[-1]}\")\n",
    "    \n",
    "# val_losses = np.array(val_losses)\n",
    "# print(val_loss.min())\n",
    "\n",
    "attention_estimator = AttentionEstimator(Dinput, Doutput)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(attention_estimator.parameters(), lr=0.001)\n",
    "\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        B, T, D = batch['obs'].shape\n",
    "        nobs = normalizer['obs'].unnormalize(batch['obs'])\n",
    "        \n",
    "        obs = nobs[:, :2, :-1].reshape(B, -1)\n",
    "        attention = nobs[:, :, -1]\n",
    "        \n",
    "        # # Reshape input to B*T, D+1 where last dim is time index\n",
    "        # obs = torch.repeat_interleave(nobs[:, :2, :-1].reshape(B, -1), T, dim=0)  # B*T, 2D\n",
    "        # time_idx = torch.arange(T, device=obs.device).repeat(B).unsqueeze(-1)  # B*T, 1\n",
    "        # # obs = torch.cat([obs, time_idx.unsqueeze(-1)], dim=-1)  # B*T, D\n",
    "        \n",
    "        # # Reshape target to B*T, 1\n",
    "        # attention = nobs[:, :, -1].reshape(-1, 1)  # B*T, 1\n",
    "        \n",
    "        output = attention_estimator(obs)\n",
    "        loss = criterion(output, attention)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "        \n",
    "    for batch in val_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        B, T, D = batch['obs'].shape\n",
    "        nobs = normalizer['obs'].unnormalize(batch['obs'])\n",
    "        \n",
    "        obs = nobs[:, :2, :-1].reshape(B, -1)\n",
    "        attention = nobs[:, :, -1]\n",
    "        \n",
    "        output = attention_estimator(obs)\n",
    "        val_loss = criterion(output, attention)\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "    print(f\"Epoch {epoch} train loss: {loss}, val loss: {val_losses[-1]}\")\n",
    "    \n",
    "val_losses = np.array(val_losses)\n",
    "print(val_loss.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6163\n",
      "0.005357782822102308\n",
      "torch.Size([1, 16]) torch.Size([1, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcfd2172a30>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iterator = iter(display_dataloader)\n",
    "# for _ in range(1223):\n",
    "#     batch = next(iterator)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     B, T, D = batch['obs'].shape\n",
    "#     nobs = normalizer['obs'].unnormalize(batch['obs'])\n",
    "    \n",
    "#     # Reshape input to B*T, D+1 where last dim is time index\n",
    "#     obs = torch.repeat_interleave(nobs[:, :2, :-1].reshape(B, -1), T, dim=0)  # B*T, 2D\n",
    "#     time_idx = torch.arange(T, device=obs.device).repeat(B).unsqueeze(-1)  # B*T, 1\n",
    "#     # obs = torch.cat([obs, time_idx.unsqueeze(-1)], dim=-1)  # B*T, D\n",
    "    \n",
    "#     # Reshape target to B*T, 1\n",
    "#     attention = nobs[:, :, -1].reshape(-1, 1)  # B*T, 1\n",
    "    \n",
    "#     output = attention_estimator(obs, time_idx)\n",
    "    \n",
    "#     print(nn.MSELoss()(output, attention).item())\n",
    "#     print(output.shape, attention.shape)\n",
    "    \n",
    "#     attention_pred = normalizer['obs'].normalize(np.concatenate([nobs[:, :, :-1], output.unsqueeze(0)], axis=-1))[:, :, -1]\n",
    "    \n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.plot(attention_pred[0])\n",
    "# plt.plot(batch['obs'][0, :, -1])\n",
    "\n",
    "iterator = iter(display_dataloader)\n",
    "idx = np.random.randint(0, len(display_dataloader))\n",
    "print(idx)\n",
    "for _ in range(idx):\n",
    "    batch = next(iterator)\n",
    "\n",
    "with torch.no_grad():\n",
    "    B, T, D = batch['obs'].shape\n",
    "    nobs = normalizer['obs'].unnormalize(batch['obs'])\n",
    "    \n",
    "    obs = nobs[:, :2, :-1].reshape(B, -1)\n",
    "    attention = nobs[:, :, -1]\n",
    "    \n",
    "    # # Reshape input to B*T, D+1 where last dim is time index\n",
    "    # obs = torch.repeat_interleave(nobs[:, :2, :-1].reshape(B, -1), T, dim=0)  # B*T, 2D\n",
    "    # time_idx = torch.arange(T, device=obs.device).repeat(B).unsqueeze(-1)  # B*T, 1\n",
    "    # # obs = torch.cat([obs, time_idx.unsqueeze(-1)], dim=-1)  # B*T, D\n",
    "    \n",
    "    # # Reshape target to B*T, 1\n",
    "    # attention = nobs[:, :, -1].reshape(-1, 1)  # B*T, 1\n",
    "    \n",
    "    output = attention_estimator(obs)\n",
    "\n",
    "\n",
    "    print(nn.MSELoss()(output, attention).item())\n",
    "    print(output.shape, attention.shape)\n",
    "    \n",
    "    attention_pred = normalizer['obs'].normalize(np.concatenate([nobs[:, :, :-1], output.unsqueeze(-1)], axis=-1))[:, :, -1]\n",
    "    \n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(attention_pred[0])\n",
    "plt.plot(batch['obs'][0, :, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

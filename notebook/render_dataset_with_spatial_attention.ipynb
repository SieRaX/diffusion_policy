{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import hydra\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import dill\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusion_policy.dataset.robomimic_replay_image_dataset import RobomimicReplayImageDataset # Need this to operate ReplayBuffer.copy_from_path. If not it will raise a codec error\n",
    "from diffusion_policy.dataset.robomimic_replay_lowdim_dataset import RobomimicReplayLowdimDataset\n",
    "from diffusion_policy.common.replay_buffer import ReplayBuffer\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.common.pytorch_util import dict_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_path = os.path.expanduser('../data/robomimic/datasets/lift/ph/image_abs.hdf5.zarr.zip')\n",
    "dataset_path = os.path.expanduser('../data/robomimic/datasets/lift/ph/image_abs.hdf5')\n",
    "replay_buffer = ReplayBuffer.copy_from_path(\n",
    "            zarr_path, keys=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_meta = {\n",
    "    'action': {\n",
    "        'shape': [7]\n",
    "    },\n",
    "    'obs': {\n",
    "        'object': {\n",
    "            'shape': [10]\n",
    "        },\n",
    "        'agentview_image': {\n",
    "            'shape': [3, 84, 84],\n",
    "            'type': 'rgb'\n",
    "        },\n",
    "        'robot0_eef_pos': {\n",
    "            'shape': [3]\n",
    "        },\n",
    "        'robot0_eef_quat': {\n",
    "            'shape': [4]\n",
    "        },\n",
    "        'robot0_eye_in_hand_image': {\n",
    "            'shape': [3, 84, 84],\n",
    "            'type': 'rgb'\n",
    "        },\n",
    "        'robot0_gripper_qpos': {\n",
    "            'shape': [2]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RobomimicReplayImageDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    shape_meta=shape_meta,\n",
    "    horizon=2,\n",
    "    pad_before=1,\n",
    "    pad_after=1,\n",
    "    rotation_rep='rotation_6d',\n",
    "    seed=42,\n",
    "    val_ratio=0.0,\n",
    "    use_legacy_normalizer=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dim_dataset = RobomimicReplayLowdimDataset(\n",
    "    dataset_path=f\"../data/robomimic/datasets/lift/ph/low_dim_abs.hdf5\",\n",
    "    horizon=2,\n",
    "    pad_before=1,\n",
    "    pad_after=1,\n",
    "    abs_action=True,\n",
    "    obs_keys=['object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dim_dataloader = DataLoader(low_dim_dataset, batch_size=1, shuffle=False)\n",
    "image_dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dim_sample = next(iter(low_dim_dataloader))\n",
    "image_sample = next(iter(image_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(low_dim_sample['obs'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs_dict = {\n",
    "            'obs': np.concatenate([image_sample['obs']['object'], image_sample['obs']['robot0_eef_pos'], image_sample['obs']['robot0_eef_quat'], image_sample['obs']['robot0_gripper_qpos']], axis=-1).astype(np.float16)\n",
    "        }\n",
    "print(n_obs_dict['obs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"../data/outputs/lift_lowdim_ph_reproduction/horizon_16/2025.03.11/10.57.22_train_diffusion_unet_lowdim_lift_lowdim_transformer_128/checkpoints/epoch=0300-test_mean_score=1.000.ckpt\"\n",
    "output_dir = \"../data/outputs/lift_lowdim_ph_reproduction/horizon_16/2025.03.11/10.57.22_train_diffusion_unet_lowdim_lift_lowdim_transformer_128/dummy\"\n",
    "payload = torch.load(open(checkpoint, 'rb'), pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "cfg.policy.noise_scheduler._target_ = 'diffusion_policy.schedulers.scheduling_ddpm.DDPMScheduler'\n",
    "\n",
    "cls = hydra.utils.get_class(cfg._target_)\n",
    "workspace = cls(cfg, output_dir=output_dir)\n",
    "workspace: BaseWorkspace\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)\n",
    "\n",
    "# get policy from workspace\n",
    "policy = workspace.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "policy.to(device)\n",
    "policy.eval()\n",
    "\n",
    "video_dir = \"../data/robomimic/datasets/lift/ph/videos_with_spatial_attention\"\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "iterator = iter(dataloader)\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(replay_buffer.n_episodes)):\n",
    "    epi = replay_buffer.get_episode(i)\n",
    "    \n",
    "    T, H, W, C = epi['agentview_image'].shape\n",
    "    \n",
    "    imgs = np.zeros((T+1, H, 2*W, C), dtype=epi['agentview_image'].dtype)\n",
    "    print(f\"episode {i}| T: {T}\")\n",
    "    \n",
    "    spatial_attention = list()\n",
    "    for t in tqdm(range(T+1), desc=\"Getting Image and Spatial Attention\", leave=False):\n",
    "        sample = next(iterator)\n",
    "        # Transpose from (3,84,84) to (84,84,3) format\n",
    "        imgs[t, :, :W, :] = ((sample['obs']['agentview_image'][0, 0].permute(1, 2, 0) * 255).numpy()).astype(np.uint8)\n",
    "        imgs[t, :, W:, :] = (sample['obs']['robot0_eye_in_hand_image'][0, 0].permute(1, 2, 0) * 255).numpy().astype(np.uint8)\n",
    "        n_obs_dict = {\n",
    "            'obs': np.concatenate([sample['obs']['object'], sample['obs']['robot0_eef_pos'], sample['obs']['robot0_eef_quat'], sample['obs']['robot0_gripper_qpos']], axis=-1).astype(np.float32)\n",
    "        }\n",
    "        \n",
    "        # device transfer\n",
    "        obs_dict = dict_apply(n_obs_dict, \n",
    "            lambda x: torch.from_numpy(x).to(\n",
    "                device=device))\n",
    "        with torch.no_grad():\n",
    "            spatial_attention.append(policy.kl_divergence_drop(obs_dict).detach().cpu().numpy().item())\n",
    "    spatial_attention = np.array(spatial_attention)\n",
    "    \n",
    "    # get attention graph images\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    graph_frames = list()\n",
    "    for t in range(len(spatial_attention)):\n",
    "        ax.scatter(t, spatial_attention[t], color='red', s=30)\n",
    "        \n",
    "        fig.canvas.draw()\n",
    "        graph_img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        graph_img = graph_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        graph_frames.append(graph_img)\n",
    "    graph_height, graph_width = graph_frames[0].shape[:2]\n",
    "    # prepare combined frames\n",
    "    \n",
    "    # attention_data = get_spatial_attention_from_episode(epi)\n",
    "    combined_frames = list()\n",
    "    for frame, graph in zip(imgs, graph_frames):\n",
    "            # Resize frame to match graph height\n",
    "            frame_height, frame_width = frame.shape[:2]\n",
    "            aspect_ratio = frame_width / frame_height\n",
    "            new_height = graph_height\n",
    "            new_width = int(new_height * aspect_ratio)\n",
    "            \n",
    "            frame_resized = np.array(Image.fromarray(frame).resize((new_width, new_height)))\n",
    "            \n",
    "            # Create canvas and center the frame\n",
    "            canvas = np.zeros((graph_height, graph_width, 3), dtype=np.uint8)\n",
    "            x_offset = (graph_width - new_width) // 2\n",
    "            \n",
    "            if x_offset >= 0:\n",
    "                canvas[:, x_offset:x_offset+new_width] = frame_resized\n",
    "            else:\n",
    "                crop_start = (-x_offset) // 2\n",
    "                canvas = frame_resized[:, crop_start:crop_start+graph_width]\n",
    "            \n",
    "            combined_frames.append(np.hstack([canvas, graph]))\n",
    "            \n",
    "    video_path = os.path.join(video_dir, f\"episode_{i}.mp4\")\n",
    "    \n",
    "    # Create and write video with moviepy\n",
    "    clip = ImageSequenceClip(combined_frames, fps=30)\n",
    "    clip.write_videofile(video_path, codec='libx264')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = \"../data/robomimic/datasets/lift/ph/videos_with_spatial_attention\"\n",
    "\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(replay_buffer.n_episodes)):\n",
    "    epi = replay_buffer.get_episode(i)\n",
    "    attention_data = get_spatial_attention_from_episode(epi)\n",
    "    video_path = os.path.join(video_dir, f\"episode_{i}.mp4\")\n",
    "    \n",
    "    # Get image sequence\n",
    "    agent_imgs = epi['agentview_image'] # T,H,W,C uint8 array\n",
    "    eye_imgs = epi['robot0_eye_in_hand_image']\n",
    "    # Concatenate images horizontally\n",
    "    imgs = np.concatenate([agent_imgs, eye_imgs], axis=2) # T,H,2W,C\n",
    "    \n",
    "    # Scale dimensions by 3x\n",
    "    h, w = imgs.shape[1:3]\n",
    "    imgs_scaled = np.array([np.array(Image.fromarray(img).resize((w*3, h*3))) for img in imgs])\n",
    "    \n",
    "    # Create and write video with moviepy\n",
    "    clip = ImageSequenceClip(list(imgs_scaled), fps=30)\n",
    "    clip.write_videofile(video_path, codec='libx264')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

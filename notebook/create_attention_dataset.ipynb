{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective: Save attention values of each timestep and save it as seperate dataset\n",
    "# 1. Load dataset\n",
    "# 2. Load model\n",
    "# 3. Get attention values\n",
    "# 4. Save attention values: Add key \"attention\" and \"previous_obs\" to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../data/robomimic/datasets/lift/ph/low_dim_abs.hdf5\"\n",
    "new_dataset_path = \"../data/robomimic/datasets/lift/ph/low_dim_abs_with_attention.hdf5\"\n",
    "\n",
    "# # Copy entire file\n",
    "# with h5py.File(dataset_path, 'r') as src, h5py.File(new_dataset_path, 'w') as dst:\n",
    "#     for key in src.keys():\n",
    "#         src.copy(key, dst)\n",
    "    \n",
    "#     print(f\"Created new dataset at {new_dataset_path}\")\n",
    "#     print(f\"Copied {len(dst.keys())} top-level groups\")\n",
    "#     demos = dst['data']\n",
    "#     print(f\"Number of demonstrations: {len(demos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "dataset_path = \"../data/robomimic/datasets/lift/ph/low_dim_abs.hdf5\"\n",
    "new_dataset_path = \"../data/robomimic/datasets/lift/ph/low_dim_abs_with_attention.hdf5\"\n",
    "\n",
    "# First, copy the entire file to preserve all structure\n",
    "shutil.copy(dataset_path, new_dataset_path)\n",
    "\n",
    "# Open the new file in read-write mode\n",
    "with h5py.File(new_dataset_path, 'r+') as f:\n",
    "    \n",
    "    num_demos = len(f['data'].keys())\n",
    "    \n",
    "    for demo_idx in range(num_demos):\n",
    "        demo_key = f'data/demo_{demo_idx}'\n",
    "        demo = f[demo_key]\n",
    "        \n",
    "        num_samples = demo.attrs['num_samples']\n",
    "        \n",
    "        for sample_idx in range(num_samples):\n",
    "            obs = demo['obs'].keys()\n",
    "            print(obs)\n",
    "        \n",
    "    # # Access the demo_0 group\n",
    "    # demo_0 = f['data/demo_0']\n",
    "    \n",
    "    # # Get the number of samples from the attribute\n",
    "    # num_samples = demo_0.attrs['num_samples']\n",
    "    \n",
    "    # # Create the spatial_attention dataset\n",
    "    # demo_0.create_dataset(\n",
    "    #     'spatial_attention',\n",
    "    #     shape=(num_samples,),  # One value per sample\n",
    "    #     dtype=np.float32,      # Using float32 for attention values\n",
    "    #     data=np.zeros(num_samples)  # Initialize with zeros\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file\n",
    "f = h5py.File(dataset_path, \"r\")\n",
    "\n",
    "# each demonstration is a group under \"data\"\n",
    "demos = list(f[\"data\"].keys())\n",
    "num_demos = len(demos)\n",
    "print(\"hdf5 file {} has {} demonstrations\".format(dataset_path, num_demos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each demonstration is a group under \"data\"\n",
    "demos_list = list(demos.keys())\n",
    "num_demos = len(demos_list)\n",
    "print(\"hdf5 file {} has {} demonstrations\".format(dataset_path, num_demos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import hydra\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import dill\n",
    "from torch.utils.data import DataLoader\n",
    "import h5py\n",
    "import shutil\n",
    "\n",
    "from diffusion_policy.dataset.robomimic_replay_image_dataset import RobomimicReplayImageDataset\n",
    "from diffusion_policy.common.replay_buffer import ReplayBuffer\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.common.pytorch_util import dict_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zarr_path = os.path.expanduser('../data/robomimic/datasets/lift/ph/image_abs.hdf5.zarr.zip')\n",
    "dataset_path = os.path.expanduser('../data/robomimic/datasets/lift/ph/image_abs.hdf5')\n",
    "# replay_buffer = ReplayBuffer.copy_from_path(zarr_path, keys=None)\n",
    "\n",
    "# Define shape metadata\n",
    "shape_meta = {\n",
    "    'action': {\n",
    "        'shape': [7]\n",
    "    },\n",
    "    'obs': {\n",
    "        'object': {\n",
    "            'shape': [10]\n",
    "        },\n",
    "        'agentview_image': {\n",
    "            'shape': [3, 84, 84],\n",
    "            'type': 'rgb'\n",
    "        },\n",
    "        'robot0_eef_pos': {\n",
    "            'shape': [3]\n",
    "        },\n",
    "        'robot0_eef_quat': {\n",
    "            'shape': [4]\n",
    "        },\n",
    "        'robot0_eye_in_hand_image': {\n",
    "            'shape': [3, 84, 84],\n",
    "            'type': 'rgb'\n",
    "        },\n",
    "        'robot0_gripper_qpos': {\n",
    "            'shape': [2]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create dataset\n",
    "dataset = RobomimicReplayImageDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    shape_meta=shape_meta,\n",
    "    horizon=2,\n",
    "    pad_before=1,\n",
    "    pad_after=1,\n",
    "    rotation_rep='rotation_6d',\n",
    "    seed=42,\n",
    "    val_ratio=0.0,\n",
    "    use_legacy_normalizer=False,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "iterator = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring h5py file\n",
    "\n",
    "# Paths\n",
    "dataset_path = \"../data/robomimic/datasets/lift/ph/low_dim_abs.hdf5\"\n",
    "new_dataset_path = \"../data/robomimic/datasets/lift/ph/low_dim_abs_with_attention.hdf5\"\n",
    "\n",
    "# First, copy the entire file to preserve all structure\n",
    "shutil.copy(dataset_path, new_dataset_path)\n",
    "\n",
    "file = h5py.File(new_dataset_path, 'r+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_demos = len(file['data'].keys())\n",
    "print(f\"Number of demonstrations: {num_demos}\")\n",
    "\n",
    "length_of_each_demo = list()\n",
    "for i in tqdm(range(num_demos)):\n",
    "    demo_key = f'data/demo_{i}'\n",
    "    demo = file[demo_key]\n",
    "    length_of_each_demo.append(demo.attrs['num_samples'])\n",
    "length_of_each_demo = np.array(length_of_each_demo)\n",
    "\n",
    "assert (length_of_each_demo+1).sum() == len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepaer model\n",
    "# Load model checkpoint\n",
    "checkpoint = \"../data/outputs/lift_lowdim_ph_reproduction/horizon_16/2025.03.11/10.57.22_train_diffusion_unet_lowdim_lift_lowdim_transformer_128/checkpoints/epoch=0200-test_mean_score=1.000.ckpt\"\n",
    "output_dir = \"../data/outputs/lift_lowdim_ph_reproduction/horizon_16/2025.03.11/10.57.22_train_diffusion_unet_lowdim_lift_lowdim_transformer_128/dummy\"\n",
    "payload = torch.load(open(checkpoint, 'rb'), pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "cfg.policy.noise_scheduler._target_ = 'diffusion_policy.schedulers.scheduling_ddpm.DDPMScheduler'\n",
    "\n",
    "cls = hydra.utils.get_class(cfg._target_)\n",
    "workspace = cls(cfg, output_dir=output_dir)\n",
    "workspace: BaseWorkspace\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)\n",
    "\n",
    "# Get policy from workspace\n",
    "policy = workspace.model\n",
    "\n",
    "# Setup device and model\n",
    "device = torch.device('cuda:0')\n",
    "policy.to(device);\n",
    "policy.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataloader)\n",
    "for i in range(1):\n",
    "    demo_key = f'data/demo_{i}'\n",
    "    demo = file[demo_key]\n",
    "    num_samples = demo.attrs['num_samples']\n",
    "    \n",
    "    spatial_attention = list()\n",
    "    for sample_idx in tqdm(range(num_samples), leave=False):        \n",
    "        sample = next(iterator)\n",
    "        \n",
    "        assert np.linalg.norm(sample['obs']['object'][0, 1, :].numpy()-demo['obs']['object'][sample_idx]) < 1e-4\n",
    "        \n",
    "        n_obs_dict = {\n",
    "                    'obs': np.concatenate([\n",
    "                        sample['obs']['object'], \n",
    "                        sample['obs']['robot0_eef_pos'], \n",
    "                        sample['obs']['robot0_eef_quat'], \n",
    "                        sample['obs']['robot0_gripper_qpos']\n",
    "                    ], axis=-1).astype(np.float32)\n",
    "                }\n",
    "        # Device transfer\n",
    "        obs_dict = dict_apply(n_obs_dict, \n",
    "            lambda x: torch.from_numpy(x).to(device=device))\n",
    "        with torch.no_grad():\n",
    "            spatial_attention.append(policy.kl_divergence_drop(obs_dict).detach().cpu().numpy().item())\n",
    "    spatial_attention = np.array(spatial_attention)\n",
    "    \n",
    "    next(iterator) # Fro syncing\n",
    "    \n",
    "    if 'spatial_attention' not in demo.keys():\n",
    "        demo.create_dataset(\n",
    "            'spatial_attention',\n",
    "            shape=(num_samples,),   \n",
    "            data=spatial_attention,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    else:\n",
    "        demo['spatial_attention'][:] = spatial_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it has the data\n",
    "file = h5py.File(new_dataset_path, 'r+')\n",
    "spatial_attention = file['data/demo_0/spatial_attention'][:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "time_steps = np.arange(len(spatial_attention))\n",
    "ax.plot(time_steps, spatial_attention, 'b-', linewidth=1, label='Spatial Attention')\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.tick_params(axis='y', labelcolor='b')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the new file in read-write mode\n",
    "with h5py.File(new_dataset_path, 'r+') as f:\n",
    "    \n",
    "    num_demos = len(f['data'].keys())\n",
    "    \n",
    "    for demo_idx in range(num_demos):\n",
    "        demo_key = f'data/demo_{demo_idx}'\n",
    "        demo = f[demo_key]\n",
    "        \n",
    "        num_samples = demo.attrs['num_samples']\n",
    "        \n",
    "        for sample_idx in range(num_samples):\n",
    "            obs = demo['obs'].keys()\n",
    "            print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
